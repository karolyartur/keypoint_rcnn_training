{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoint detection with Pytorch"
      ],
      "metadata": {
        "id": "1CHvepRs60vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and installs"
      ],
      "metadata": {
        "id": "jl5PX4b9Xctl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "import datetime\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pycocotools.cocoeval import COCOeval"
      ],
      "metadata": {
        "id": "tE02xYtW6-UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the updated Torchvision repo and install Trochvision from source\n",
        "!git clone https://github.com/karolyartur/vision\n",
        "!pip uninstall torchvision -y\n",
        "!cd vision && python setup.py install\n",
        "\n",
        "# Add the built torchvision package to sys.path\n",
        "sys.path.append('vision/build/lib.linux-x86_64-3.8')"
      ],
      "metadata": {
        "id": "60Dw21EBm7o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the newly built torchvision and necessary modules\n",
        "import torchvision\n",
        "\n",
        "sys.path.append('vision/references/detection')\n",
        "\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.keypoint_rcnn import KeypointRCNNPredictor\n",
        "\n",
        "# Import modules from vision/references\n",
        "import vision.references.detection.utils as utils\n",
        "import vision.references.detection.engine as engine\n",
        "from vision.references.detection.utils import collate_fn\n",
        "from vision.references.detection.engine import train_one_epoch, _get_iou_types\n",
        "from vision.references.detection.coco_utils import get_coco_api_from_dataset\n",
        "from vision.references.detection.coco_eval import CocoEvaluator"
      ],
      "metadata": {
        "id": "PtHX6eI0kxav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Constants and function definitions"
      ],
      "metadata": {
        "id": "t1zr3Oz5Xsjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants and functions\n",
        "\n",
        "DATASET_PATH = 'drive/MyDrive/KK/SlideBot/data/synthetic_data_keypoints'\n",
        "SEGMENTATION_PATH = os.path.join(DATASET_PATH, 'Segmentation_annotations')\n",
        "TRAIN_IMGS_PATH = os.path.join(DATASET_PATH, 'datasets', 'images', 'train')\n",
        "VAL_IMGS_PATH = os.path.join(DATASET_PATH, 'datasets', 'images', 'val')\n",
        "KEYPOINT_ANNOT_PATH = os.path.join(DATASET_PATH, 'keypoint_annotations_coco')\n",
        "MODEL_OUTPUT_PATH = 'drive/MyDrive/KK/SlideBot/data/training_results/keypoint_rcnn'\n",
        "BOX_COLOR = [255,0,0,255]\n",
        "BG_COLOR = [0,0,0,255]\n",
        "\n",
        "NUM_KEYPOINTS = 48\n",
        "\n",
        "train_imgs = os.listdir(TRAIN_IMGS_PATH)\n",
        "val_imgs = os.listdir(VAL_IMGS_PATH)\n",
        "train_imgs.sort()\n",
        "val_imgs.sort()\n",
        "all_image_files = copy.copy(val_imgs)\n",
        "all_image_files.extend(train_imgs)\n",
        "\n",
        "def img_name_to_annot_name(img_name):\n",
        "  '''Return the name of the annotation image given the name of the rendered image\n",
        "  '''\n",
        "  return img_name.replace('.', '_annotation.')\n",
        "\n",
        "def visualize_annot(img, bboxes, keypoints, threshold=0.5):\n",
        "  '''Visualize bounding box and keypoint annotations\n",
        "  '''\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(img)\n",
        "  for index,bbox in enumerate(bboxes):\n",
        "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=2, edgecolor='g', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    for id, keypoint in enumerate(keypoints[index]):\n",
        "      if keypoint[2] >= threshold:\n",
        "        circ = patches.Circle((keypoint[0], keypoint[1]), radius=2, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(circ)\n",
        "        ax.text(keypoint[0], keypoint[1], str(id), color='r', fontsize=10)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "1tqScJTi7KRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create COCO-style annotations from segmentation masks"
      ],
      "metadata": {
        "id": "kI2TEUZ3YT5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data availability\n",
        "\n",
        "img = np.array(Image.open(os.path.join(TRAIN_IMGS_PATH, train_imgs[0])))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = np.array(Image.open(os.path.join(SEGMENTATION_PATH, img_name_to_annot_name(train_imgs[0]))))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = np.array(Image.open(os.path.join(VAL_IMGS_PATH, val_imgs[0])))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = np.array(Image.open(os.path.join(SEGMENTATION_PATH, img_name_to_annot_name(val_imgs[0]))))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l94-fNiO7_TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique colors (This is separated because it takes a lot of time)\n",
        "\n",
        "expected_unique_color_num = 2 + NUM_KEYPOINTS/2  # Background + Box + 24 slide\n",
        "\n",
        "segment_imgs = os.listdir(SEGMENTATION_PATH)\n",
        "segment_imgs.sort()\n",
        "\n",
        "unique_colors = []\n",
        "\n",
        "for filename in segment_imgs:\n",
        "  img = np.array(Image.open(os.path.join(SEGMENTATION_PATH, filename)))\n",
        "  img = np.reshape(img, (img.shape[0]*img.shape[1], img.shape[2]))\n",
        "  for color in np.unique(img, axis=0):\n",
        "    if color.tolist() not in unique_colors:\n",
        "      unique_colors.append(color.tolist())\n",
        "  if len(unique_colors) >= expected_unique_color_num:\n",
        "    break"
      ],
      "metadata": {
        "id": "zetuwb0rBncT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate bbox and keypoint annotation JSON files for the train and valid images\n",
        "\n",
        "unique_colors.sort()\n",
        "slide_colors = [color for color in unique_colors if not color in [BOX_COLOR, BG_COLOR]]\n",
        "\n",
        "def mask_img_to_bbox(mask_img, color):\n",
        "  '''Return a bounding box [x_min, y_min, x_max, y_max] given an input segmentation mask and a color ID\n",
        "  '''\n",
        "  xs, ys = np.where(np.all(mask_img==color, axis=2))\n",
        "  if len(xs) > 0 and len(ys) > 0:\n",
        "    return([int(val) for val in [min(xs), min(ys), max(xs), max(ys)]])\n",
        "  else:\n",
        "    return(None)\n",
        "\n",
        "\n",
        "def mask_image_to_keypoints(img, color_ids, bbox):\n",
        "  '''Return list of keypoints given a segmentation image, the color ID-s for the keypoints and the bounding box for the containing instance\n",
        "\n",
        "  For each color is color_id two keypoints will be created one for the left side of the bounding box and one for the right side\n",
        "  '''\n",
        "  bbox_middle = (bbox[1]+bbox[3])/2\n",
        "  keypoints = []\n",
        "  for slide_color in color_ids:\n",
        "    selected_pixels = np.where(np.all(img==slide_color, axis=2))\n",
        "    missing = len(selected_pixels[0])==0 and len(selected_pixels[1])==0\n",
        "    if missing:\n",
        "      keypoints.append([0,0,0])  # Left keypoint\n",
        "      keypoints.append([0,0,0])  # Right keypoint\n",
        "    else:\n",
        "      if not (np.all(selected_pixels[1] < bbox_middle) or np.all(selected_pixels[1] > bbox_middle)) and max(selected_pixels[1])-min(selected_pixels[1]) > 50:\n",
        "        # Both keypoints for the slide are visible\n",
        "        left_pixels = np.where(selected_pixels[1] < bbox_middle)\n",
        "        right_pixels = np.where(selected_pixels[1] > bbox_middle)\n",
        "        keypoints.append([int(np.mean(selected_pixels[0][left_pixels])),int(np.mean(selected_pixels[1][left_pixels])),1])  # Left keypoint\n",
        "        keypoints.append([int(np.mean(selected_pixels[0][right_pixels])),int(np.mean(selected_pixels[1][right_pixels])),1])  # Right keypoint\n",
        "      elif np.all(selected_pixels[1] < bbox_middle) or bbox[3]==img.shape[1]-1:\n",
        "        # Only left side of the slide is visible\n",
        "        keypoints.append([int(np.mean(selected_pixels[0])),int(np.mean(selected_pixels[1])),1])  # Left keypoint\n",
        "        keypoints.append([0,0,0])  # Right keypoint\n",
        "      elif np.all(selected_pixels[1] > bbox_middle) or bbox[1] == 0:\n",
        "        # Only right side of the slide is visible\n",
        "        keypoints.append([0,0,0])  # Left keypoint\n",
        "        keypoints.append([int(np.mean(selected_pixels[0])),int(np.mean(selected_pixels[1])),1])  # Right keypoint\n",
        "  return keypoints\n",
        "\n",
        "for img_file_name in all_image_files:\n",
        "  print(img_file_name)\n",
        "  mask_img = np.array(Image.open(os.path.join(SEGMENTATION_PATH, img_name_to_annot_name(img_file_name))))\n",
        "  bbox = mask_img_to_bbox(mask_img, BOX_COLOR)\n",
        "  if bbox:\n",
        "    keypoints = mask_image_to_keypoints(mask_img, slide_colors, bbox)\n",
        "\n",
        "  json_name = img_file_name.split('.')[0] + '.json'\n",
        "  with open(os.path.join(KEYPOINT_ANNOT_PATH, json_name), 'w') as f:\n",
        "    if bbox:\n",
        "      bbox = [bbox[1],bbox[0],bbox[3],bbox[2]]\n",
        "      keypoints_copy = copy.deepcopy(keypoints)\n",
        "      for i in range(len(keypoints_copy)):\n",
        "        keypoints[i][0] = keypoints_copy[i][1]\n",
        "        keypoints[i][1] = keypoints_copy[i][0]\n",
        "      f.write(json.dumps({'bboxes':[bbox], 'keypoints':[keypoints]}))\n",
        "    else:\n",
        "      f.write(json.dumps({'bboxes':[], 'keypoints':[]}))"
      ],
      "metadata": {
        "id": "LdpgVJRKLHuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keypoint detector training"
      ],
      "metadata": {
        "id": "F_FrI35ckUIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset definition"
      ],
      "metadata": {
        "id": "6Nbb6h3FYnpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path, annot_path, num=None, size=(720,1280), num_keypoints=48):                \n",
        "        self.size=size\n",
        "        self.annotations_files = [os.path.join(annot_path, e) for e in sorted(os.listdir(annot_path))]\n",
        "        self.imgs_files = [os.path.join(img_path, e) for e in sorted(os.listdir(img_path))]\n",
        "        if num:\n",
        "          if not isinstance(num, int):\n",
        "            raise TypeError(f'\"num\" argument must be integer, instead got {type(num)}')\n",
        "          if num < len(self.imgs_files):\n",
        "            self.imgs_files = self.imgs_files[0:num]\n",
        "            self.annotations_files = [e for e in self.annotations_files if any(os.path.split(e)[-1].split('.')[0] in p for p in self.imgs_files)]\n",
        "          else:\n",
        "            warnings.warn(f'\"num\" is greater than the number of images in the dataset ({len(self.imgs_files)}) all images will be used!') \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs_files[idx]\n",
        "        annotations_path = self.annotations_files[idx]\n",
        "\n",
        "        img = np.array(Image.open(img_path))[:,:,:3]\n",
        "        img = cv2.resize(img, (self.size[1], self.size[0]), interpolation=cv2.INTER_LINEAR)\n",
        "        \n",
        "        bbox_exists = False\n",
        "        with open(annotations_path) as f:\n",
        "            data = json.loads(f.read())\n",
        "            bboxes = data['bboxes']\n",
        "            if bboxes:\n",
        "              bboxes[0][0] *= self.size[1]/1920\n",
        "              bboxes[0][2] *= self.size[1]/1920\n",
        "              bboxes[0][1] *= self.size[0]/1080\n",
        "              bboxes[0][3] *= self.size[0]/1080\n",
        "              bbox_exists = True\n",
        "            keypoints = data['keypoints']\n",
        "            if keypoints:\n",
        "              for i in range(len(keypoints[0])):\n",
        "                keypoints[0][i][0] *= self.size[1]/1920\n",
        "                keypoints[0][i][1] *= self.size[0]/1080\n",
        "        \n",
        "        # Convert everything into a torch tensor        \n",
        "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
        "        target = {}\n",
        "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are boxes\n",
        "        target[\"image_id\"] = torch.tensor([idx])\n",
        "        if bbox_exists:\n",
        "          target[\"boxes\"] = bboxes\n",
        "          target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
        "          target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
        "          target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "        else:\n",
        "          target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
        "          target[\"area\"] = torch.tensor([0])\n",
        "          target[\"iscrowd\"] = torch.ones(len(bboxes), dtype=torch.int64)\n",
        "          target[\"keypoints\"] = torch.zeros((1, num_keypoints, 3), dtype=torch.float32)        \n",
        "        img = F.to_tensor(img)        \n",
        "        return img, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs_files)"
      ],
      "metadata": {
        "id": "AqO1P28bkZeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset and annotations\n",
        "\n",
        "demo = False\n",
        "\n",
        "train_dataset = CustomDataset(TRAIN_IMGS_PATH, KEYPOINT_ANNOT_PATH, num=1, num_keypoints=NUM_KEYPOINTS)\n",
        "data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "iterator = iter(data_loader)\n",
        "batch = next(iterator)\n",
        "print(\"Targets:\\n\", batch[1])\n",
        "\n",
        "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
        "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
        "keypoints = batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist()\n",
        "\n",
        "visualize_annot(image, bboxes, keypoints)"
      ],
      "metadata": {
        "id": "d7HArxiu9otm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation definition"
      ],
      "metadata": {
        "id": "1rQsuggXYro8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device, keypointnum=48, print_freq=100):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = \"Test:\"\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types, keypointnum=keypointnum)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "metadata": {
        "id": "aj92NpzlW0rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Vb2Vf730Yu4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(num_keypoints, weights_path=None):\n",
        "  anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
        "  model = torchvision.models.detection.keypointrcnn_resnet50_fpn(\n",
        "      pretrained=False,\n",
        "      pretrained_backbone=True,\n",
        "      num_keypoints=num_keypoints,\n",
        "      num_classes = 2, # Background is the first class, box is the second class\n",
        "      rpn_anchor_generator=anchor_generator)\n",
        "  model.roi_heads.keypoint_predictor = KeypointRCNNPredictor(512,num_keypoints)\n",
        "  if weights_path:\n",
        "    state_dict = torch.load(weights_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "  return model\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "# Create datasets and data loaders\n",
        "dataset_train = CustomDataset(TRAIN_IMGS_PATH, KEYPOINT_ANNOT_PATH, num=4, num_keypoints=NUM_KEYPOINTS)\n",
        "dataset_val = CustomDataset(VAL_IMGS_PATH, KEYPOINT_ANNOT_PATH, num=4, num_keypoints=NUM_KEYPOINTS)\n",
        "\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Create model\n",
        "model = get_model(num_keypoints = NUM_KEYPOINTS)\n",
        "model.to(device)\n",
        "\n",
        "# Set otpimizer and hyperparams\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = 100\n",
        "\n",
        "#Trainig loop\n",
        "print_freq=50\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=print_freq)\n",
        "    evaluate(model, data_loader_val, device, keypointnum=NUM_KEYPOINTS, print_freq=print_freq)\n",
        "    \n",
        "# Save model weights after training\n",
        "torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_PATH, f\"keypointrcnn_weights{str(datetime.datetime.now()).split('.')[0].replace(' ','_')}.pth\"))"
      ],
      "metadata": {
        "id": "y-eeTECaccbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions"
      ],
      "metadata": {
        "id": "rX5Abk6ZY6-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image from validation set\n",
        "iterator = iter(data_loader_val)\n",
        "images, targets = next(iterator)\n",
        "images_copy = copy.copy(images)\n",
        "images = [image.to(device) for image in images]\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    output = model(images)\n",
        "\n",
        "# Visualize predicitions\n",
        "print(\"Predictions: \\n\", output)\n",
        "visualize_annot(images_copy[0].permute(1,2,0), [output[0]['boxes'].cpu()[0]], output[0]['keypoints'].cpu())"
      ],
      "metadata": {
        "id": "fbB_rSF2Zk2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}